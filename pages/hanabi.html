<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Hanabi</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../styles/styles.css">
    <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
</head>
<body>

<div class='mainPanel'>

    <div class='pageTitle'>Hanabi</div>

    <div id='navBar' class='navBar'>
        <button class='navButton' id='navHome'>Home</button>
        <button class='navButton' id='navProjects'>Projects</button>
        <button class='navButton' id='navLiterature'>Literature</button>
        <button class='navButton' id='navResume'>Resume</button>
    </div>

    <div class="centeredContent">

        <div class="lightPanel">

            <img class="centeredImage" src="../images/human_play.png"/>

            <div class="caption">
                My Hanabi agent, "Cyclone" (top bar), won the Johns Hopkins Applied Physics Laboratory's (APL) Learning
                to Read Minds challenge by achieving the highest scores when paired with human teammates. This was an
                exciting result, because human-play scores above 16 points had not been reported in literature at the
                time of the competition. This figure is from
                <a ref="https://arxiv.org/pdf/2111.01726.pdf">our paper</a>
                detailing the agent's design, development, and performance.
            </div>

            <div class="sectionTitle">
                Hanabi and the Learning to Read Minds Challenge
            </div>
            <p>
                In 2019, DeepMind marked Hanabi as <a href="https://arxiv.org/abs/1902.00506">
                a new frontier for AI research</a>. <a href="https://en.wikipedia.org/wiki/Hanabi_(card_game)">Hanabi</a>
                is a collaborative card game where players can see other players' cards, but not their own. In order to
                make valid plays, players much exchange clues with one another about which cards they hold, but these
                clues are regulated, costly, and contain partial information. Consequently, Hanabi is a game of nuance;
                successful Hanabi players can take subtle hints from how other players behave, and effectively picking
                up on these hints is often the key to success.
            </p>
            <div class="sectionSubtitle">Why Is Hanabi an AI Frontier?</div>
            <p>
                Several Hanabi agents exist that <a href="https://venturebeat.com/2019/12/06/facebooks-hanabi-playing-ai-achieves-state-of-the-art-results/">
                excel at the game</a>, but only when pair with an identical copy of themselves (called "self-play"). This is a huge advantage,
                since any agent playing with a copy of itself is guaranteed to know any conventions, nuances, or implications
                behind the actions of their twin. But how do these agents play with different teammates, teammates they
                may have never met before? This was the central question of APL's Learning to Read Minds research effort (paper in development),
                an effort which uncovered that these self-play bots perform very poorly when paired with non-identical teammates
                (as well as when paired with a human teammate). Importantly, this reveals that AIs that excel a a task
                like Hanabi might be ill-suited to collaborate with humans, and this raises the need for alternative training
                methods in order to develop AIs that can collaborate effectively with human teammates.
            </p>
            <div class="sectionSubtitle">The APL Hanabi Challenge</div>
            <p>
                In the summer of 2021, the Johns Hopkins Applied Physics Laboratory (APL) held a challenge that tasked
                staff to develop AI agents that would play well with human teammates in Hanabi. These agents would have
                to compete with each other as well as bots developed by
                <a href="https://arxiv.org/abs/2004.13291">DeepMind</a>,
                <a href="https://arxiv.org/abs/1912.02288">Facebook</a>, and
                <a href="https://github.com/lightvector/fireflower">academia</a>.
                In all, 8 agents competed for the highest scores when playing with a human teammate. My agent, "Cyclone,"
                emerged as the winner of the challenge by a comfortable margin (mean score 4 points higher than 2nd place).
            </p>

            <p>
                The key to Cyclone's success was a training process that allowed Cyclone to discover a play style that
                was complementary to the human play style. To accomplish this, I set the initial goal of developing Cyclone
                to play as much like a human as possible. This involved coding Cyclone to attend to factors in the game
                that humans are known to attend to (e.g. the number of info tokens). Then, Cyclone searched for the weights
                for these factors that maximized the accuracy with which Cyclone could predict the move a human would make
                in a game state (ultimately with an accuracy of about 70%).
            </p>
            <p>
                At this point, Cyclone could emulate human decision making with fairly high (70%) accuracy. This created
                an opportunity to simulate a human teammate with which Cyclone could play many (more than 500,000) games during which
                the non-simulated-human version of Cyclone explored different weights on the latent factors that led to
                higher scores for the Cyclone pair. In this way, Cyclone was able to discover a play style that was likely
                to complement the human play style.
            </p>

            <div class="sectionSubtitle"></div>

            <img class="centeredImage" src="../images/humanness.png" />

            <div class="caption">Figure 1. The "humanness" of Cyclone increased for the most part during development,
            until an important transition towards the end of development (version 7 to 8) where Cyclone was finally
            allowed to deviate from emulating human play in order to find a play style that complemented human play.
            Figure is from <a ref="https://arxiv.org/pdf/2111.01726.pdf">our paper</a>.
            </div>

            <img class="centeredImage" src="../images/selfScore.png" />

            <div class="caption">Figure 2. The self-play scores of Cyclone increased for the most part during development.
            The self play score for version 7 is noticeably lower because from version 6 to 7 Cyclone was seeking better
            accuracy in predicting human decisions as opposed to seeking a higher self-play score. Interestingly, Cyclone's
            self play score from version 6 to version 8 is lower, yet version 8 achieves higher scores when playing with
            humans than version 6. This indicates that version 8 of Cyclone isn't playing Hanabi <i>better</i> than version 6,
            rather that version 8 is playing Hanabi in a more human complimentary way than version 6.
            Figure is from <a ref="https://arxiv.org/pdf/2111.01726.pdf">our paper</a>.
            </div>

            <div class="sectionSubtitle">Results</div>
            <p>
                My agent achieved an average score with a human teammate (16.5) that outperformed the top human teammate Hanabi
                agents in literature (average score of 15.8) [1][2]. Not only did the agent achieve high scores when paired with
                a human teammate, but this arrangement (Cyclone + a human) achieved higher scores than the best self-play Cyclone
                agent. This reveals that asymmetric strategies across team members can not only lead to better <i>ad hoc</i>
                teaming results, but that asymmetric strategies may themselves outperform symmetric strategies.  This opens
                an exciting door to new optimization methods that go beyond the traditional self-play paradigm of recent
                reinforcement learning agent accomplishments.
            </p>

            <img class="centeredImage" src="../images/cross_play.png"/>

            <div class="caption">
                This cross play matrix shows the average score obtained by different combinations of Cyclone agent version
                (and humans). Note that the human complementary version of Cyclone achieved a higher average score with
                a human teammate than human-human pairs achieved (indicating that Cyclone is better at teaming with humans
                <i>than</i> humans). Additionally, this matrix shows that certain asymmetric teams achieved better performance
                than any self-play team, indicating that self-play optimization methods might be limited in their ability
                to produce high quality performance for <i>ad hoc</i> teaming scenarios.
            </div>

            <div class="sectionSubtitle">References</div>
            <p>[1] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jackob Foerster,
                <a href="https://arxiv.org/abs/2003.02979">"'Other-play' for zero-shot coordination", 2021</a></p>
            <p>[2] Ho Chit Siu, Jaime D. Pena, Edenna Chen, Yutai Zhou, Victor J. Lopez, Kyle Palko, Kimerlee C. Chang,
            and Ross E. Allen,
                <a href="https://arxiv.org/abs/2107.07630">"Evaulation of human-AI teams for learned and rule-based
                    agents in Hanabi"</a>, 2021</p>

        </div>

    </div>
</div>

<script src="../scripts/procedurals.js"></script>
</body>
</html>